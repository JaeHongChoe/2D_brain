{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd5fa43-9cb3-483c-8c1e-d0b478b33fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL \n",
    "import urllib\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from random import uniform\n",
    "from imgaug import augmenters as iaa\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf617e5-0b6c-4076-83c7-d309c84e66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('./Train/name_mapping.csv')\n",
    "val_csv = pd.read_csv('./Val/name_mapping_validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8542171-151b-4d78-a513-2488e6285394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS_2020_subject_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS20_Training_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS20_Training_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS20_Training_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS20_Training_004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS20_Training_005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BraTS_2020_subject_ID\n",
       "0  BraTS20_Training_001\n",
       "1  BraTS20_Training_002\n",
       "2  BraTS20_Training_003\n",
       "3  BraTS20_Training_004\n",
       "4  BraTS20_Training_005"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366b8b17-b43e-4369-8821-3788999e0f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS_2020_subject_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS20_Validation_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS20_Validation_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS20_Validation_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS20_Validation_004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS20_Validation_005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BraTS_2020_subject_ID\n",
       "0  BraTS20_Validation_001\n",
       "1  BraTS20_Validation_002\n",
       "2  BraTS20_Validation_003\n",
       "3  BraTS20_Validation_004\n",
       "4  BraTS20_Validation_005"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9915c0-bb07-4155-851e-8a8e404ae333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wonjin\\\\Documents\\\\python_conda\\\\brainMRI'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30549a2-d90a-44d9-b143-f174f2120fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c280bb-b39a-4762-b81f-aeeff949b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_flair.nii.gz'\n",
    "train_label= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_seg.nii.gz'\n",
    "# val_data= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_flair.nii.gz'\n",
    "# val_lavel= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_seg.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfba85f-d0da-4ee4-8b8d-86b59d227670",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c609cd3528a4d57b775c736bed29e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wonjin\\AppData\\Local\\Temp/ipykernel_28848/2690741212.py:2: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  x_img = nib.load(train_data[c]).get_data()\n",
      "C:\\Users\\wonjin\\AppData\\Local\\Temp/ipykernel_28848/2690741212.py:3: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  y_img = nib.load(train_label[c]).get_data()\n"
     ]
    }
   ],
   "source": [
    "#for c in tqdm(range(369)):\n",
    "    #x_img = nib.load(train_data[c]).get_data()\n",
    "    #y_img = nib.load(train_label[c]).get_data()\n",
    "    #many = len(x_img[0][0])\n",
    "    #os.mkdir('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c])\n",
    "    #for i in range(many):\n",
    "        #np.save('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c]+'/'+f'train_{i}', x_img.T[i])\n",
    "        #np.save('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c]+'/'+f'label_{i}', y_img.T[i])\n",
    "    # 240, 240, 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e328ca-de8f-4b81-a366-18fb958062c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "train_data= sorted(glob.glob('./cut_img/**/train_*.npy'))\n",
    "train_label= sorted(glob.glob('./cut_img/**/label_*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dea39361-8570-4849-b1f9-3a1ae5f9e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "aug = A.Compose([\n",
    "                    A.Resize(512,512)\n",
    "            \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "979b8c40-aad3-4888-aede-c13a8a2e3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_dir, y_dir,aug):\n",
    "        super().__init__()\n",
    "        self.x_img = x_dir\n",
    "        self.y_img = y_dir   \n",
    "        self.transform=aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_img = self.x_img[idx]\n",
    "        y_img = self.y_img[idx]\n",
    "        \n",
    "        x_img = np.load(x_img, allow_pickle=True)\n",
    "        y_img = np.load(y_img, allow_pickle=True)\n",
    "        \n",
    "        x_img = np.array(x_img, dtype=np.float64)\n",
    "        y_img = np.array(y_img, dtype=np.float64)\n",
    "        \n",
    "        x_img = np.expand_dims(x_img, axis=2)\n",
    "        y_img = np.expand_dims(y_img, axis=2)   # (512, 512, 1)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=x_img,mask=y_img)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        img=np.transpose(img,(2,0,1))\n",
    "        mask=np.transpose(mask,(2,0,1))\n",
    "        return img,mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faa9b755-7dac-48ca-924a-a09017f20f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data,train_label,aug)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67ffcc8e-4c10-48bd-95a5-f52262a514ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 512, 512])\n",
      "torch.Size([8, 1, 512, 512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 512, 512) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29420/1994770933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject1\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         data=None, **kwargs):\n\u001b[1;32m-> 2903\u001b[1;33m     __ret = gca().imshow(\n\u001b[0m\u001b[0;32m   2904\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject1\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject1\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5609\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject1\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    707\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 709\u001b[1;33m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[0;32m    710\u001b[0m                             .format(self._A.shape))\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (1, 512, 512) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##input과 label이 맞나 확인\n",
    "images,labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(images[0],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca4434c5-17f1-4185-9464-6215afea04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_channel_dice(input, target, epsilon=1e-5, ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order).contiguous()\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.view(C, -1)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Computes Dice Loss, which just 1 - DiceCoefficient described above.\n",
    "    Additionally allows per-class weights to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True,\n",
    "                 skip_last_target=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if isinstance(weight, list):\n",
    "            weight = torch.Tensor(weight)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        # if True skip the last channel in the target\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "\n",
    "        input = self.normalization(input)\n",
    "        if self.weight is not None:\n",
    "            weight = Variable(self.weight, requires_grad=False).to(input.device)\n",
    "        else:\n",
    "            weight = None\n",
    "\n",
    "        if self.skip_last_target:\n",
    "            target = target[:, :-1, ...]\n",
    "\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index, weight=weight)\n",
    "        # Average the Dice score across all channels/classes\n",
    "        return torch.mean(1. - per_channel_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cb9e0a4-f8e1-41b4-bdc4-fac940f7a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.FPN(  #DeepLabV3\n",
    "    encoder_name=\"resnet34\",# choose encoder, e.g. mobilenet_v2 or efficientnet-b7 resnext101_32x8d,timm-res2net101_26w_4s     # use `imagenet` pre-trained weights for encoder initialization \n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,\n",
    "    # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5e2fb92-3875-4323-b46d-c673d0c07479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23149121"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0f866fc-cba5-4414-83e7-ae8c43ea0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion =  DiceLoss(sigmoid_normalization=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-8, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db39b8-8262-4ac7-a6c8-9f8f4067f648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38d3a0afec24c539c0b3c0872f85368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "cnt =0\n",
    "train_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# keep track of training and validation loss\n",
    "train_loss = torch.zeros(n_epochs)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "for e in range(0, n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, labels = data.float().to(device), labels.float().to(device) #cpu에 있는 데이터를 gpu에 보냄\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        logits = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss[e] += loss.item()\n",
    "       \n",
    "    train_loss[e] /= len(train_loader)\n",
    "    \n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "    train_loss_min,\n",
    "    train_loss[e]))\n",
    "    torch.save(model.state_dict(), 'model_best_1.pt')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfda72-e322-4102-b9b5-2e258caa72e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
