{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL \n",
    "import urllib\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from random import uniform\n",
    "from imgaug import augmenters as iaa\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db77c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('./Train/name_mapping.csv')\n",
    "val_csv = pd.read_csv('./Val/name_mapping_validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e84d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58413ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb37d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd87ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_flair.nii.gz'\n",
    "train_label= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_seg.nii.gz'\n",
    "# val_data= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_flair.nii.gz'\n",
    "# val_lavel= './Train/'+train_csv['BraTS_2020_subject_ID']+'/'+train_csv['BraTS_2020_subject_ID']+'_seg.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea83cdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for c in tqdm(range(369)):\n",
    "    #x_img = nib.load(train_data[c]).get_data()\n",
    "    #y_img = nib.load(train_label[c]).get_data()\n",
    "    #many = len(x_img[0][0])\n",
    "    #os.mkdir('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c])\n",
    "    #for i in range(many):\n",
    "        #np.save('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c]+'/'+f'train_{i}', x_img.T[i])\n",
    "        #np.save('./cut_img/'+train_csv['BraTS_2020_subject_ID'][c]+'/'+f'label_{i}', y_img.T[i])\n",
    "    # 240, 240, 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "train_data= sorted(glob.glob('./cut_img/**/train_*.npy'))\n",
    "train_label= sorted(glob.glob('./cut_img/**/label_*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "aug = A.Compose([\n",
    "                    A.Resize(512,512)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(img, lower, upper):\n",
    "    return (lower <= img) & (img <= upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_dir, y_dir,aug):\n",
    "        super().__init__()\n",
    "        self.x_img = x_dir\n",
    "        self.y_img = y_dir   \n",
    "        self.transform=aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_img = self.x_img[idx]\n",
    "        y_img = self.y_img[idx]\n",
    "        \n",
    "        x_img = np.load(x_img, allow_pickle=True)\n",
    "        y_img = np.load(y_img, allow_pickle=True)\n",
    "        \n",
    "        x_img = np.array(x_img, dtype=np.float64)\n",
    "        y_img = np.array(y_img, dtype=np.float64)\n",
    "        \n",
    "        x_img = np.expand_dims(x_img, axis=2)\n",
    "        color_im = np.zeros([240,240,4])\n",
    "        for i in range(1,5):\n",
    "            encode_ = to_binary(y_img, i*1.0, i*1.0)*255\n",
    "            color_im[:, :,i-1] = encode_\n",
    "        #y_img = np.expand_dims(y_img, axis=2)   # (512, 512, 1)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=x_img,mask=color_im)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        img=np.transpose(img,(2,0,1))\n",
    "        mask = np.transpose(mask,(2,0,1))\n",
    "        #mask=np.transpose(mask,(2,0,1))\n",
    "        y_img = np.expand_dims(y_img, axis=2)\n",
    "        return img,mask,y_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data,train_label,aug)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=12,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce527b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.where(y[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0c66e9a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "34a86c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0][180][335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e3ef8db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6b4e87b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29196/3128940560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "plt.imshow(labels[0].T[3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dec23656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 512, 512])\n",
      "torch.Size([12, 4, 512, 512])\n",
      "torch.Size([12, 512, 512, 1])\n",
      "torch.Size([12, 240, 240, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##input과 label이 맞나 확인\n",
    "images,labels,y = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "images=np.transpose(images,(0,2,3,1))\n",
    "labels=np.transpose(labels,(0,2,3,1))\n",
    "\n",
    "print(images.shape)\n",
    "print(y.shape)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(images[0],cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(labels[0])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(y[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bfbb595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_channel_dice(input, target, epsilon=1e-5, ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order).contiguous()\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.view(C, -1)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Computes Dice Loss, which just 1 - DiceCoefficient described above.\n",
    "    Additionally allows per-class weights to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True,\n",
    "                 skip_last_target=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if isinstance(weight, list):\n",
    "            weight = torch.Tensor(weight)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        # if True skip the last channel in the target\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "\n",
    "        input = self.normalization(input)\n",
    "        if self.weight is not None:\n",
    "            weight = Variable(self.weight, requires_grad=False).to(input.device)\n",
    "        else:\n",
    "            weight = None\n",
    "\n",
    "        if self.skip_last_target:\n",
    "            target = target[:, :-1, ...]\n",
    "\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index, weight=weight)\n",
    "        # Average the Dice score across all channels/classes\n",
    "        return torch.mean(1. - per_channel_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba1f4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.FPN(  #DeepLabV3\n",
    "    encoder_name=\"resnet34\",# choose encoder, e.g. mobilenet_v2 or efficientnet-b7 resnext101_32x8d,timm-res2net101_26w_4s     # use `imagenet` pre-trained weights for encoder initialization \n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,\n",
    "    # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=4,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91518277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23149508"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7615bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion =  DiceLoss(sigmoid_normalization=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-8, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ccd35afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e052089d6a4a72b1976fd56e575183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> -0.319226).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c22cb507899407d819896f6261e15cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29196/635042157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#cpu에 있는 데이터를 gpu에 보냄\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject1\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    190\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "cnt =0\n",
    "train_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# keep track of training and validation loss\n",
    "train_loss = torch.zeros(n_epochs)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "for e in range(0, n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, labels = data.float().to(device), labels.float().to(device) #cpu에 있는 데이터를 gpu에 보냄\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        logits = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss[e] += loss.item()\n",
    "       \n",
    "    train_loss[e] /= len(train_loader)\n",
    "    \n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "    train_loss_min,\n",
    "    train_loss[e]))\n",
    "    torch.save(model.state_dict(), 'model_best_1.pt')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef7917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
